{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dfe4912-87d6-4fb3-93ea-a74747662665",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bb3e93-47d2-4c83-ac09-428e1f06ad97",
   "metadata": {},
   "source": [
    "## Flujos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d93c6a-5918-40c2-8e71-10993ba3c9b4",
   "metadata": {},
   "source": [
    "* Es continuo, la frecuencia de la llegada de los datos depende del problema\n",
    "* Datos son recolectados en tiempo real\n",
    "* No se almacenan para entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d0453b-9e2c-4ff5-a472-b0c4671a4bec",
   "metadata": {},
   "source": [
    "## Fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2577d88-6a34-4cbe-9f4f-a922a6e198e4",
   "metadata": {},
   "source": [
    "* RRSS\n",
    "* Transacciones bancarias o criptomonedas\n",
    "* Monitoreos de redes, sensores\n",
    "* Análisis climático\n",
    "* Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc7eb61-cc6a-4e6d-ab23-ca2f7f9dd2bb",
   "metadata": {},
   "source": [
    "## Estrategias para el tratamiento del flujo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76c30b-7484-4fc5-82e8-18920dec7667",
   "metadata": {},
   "source": [
    "* El dato se recibe, se utiliza y se descarta\n",
    "* Ventana temporal para guardar los últimos n datos recibidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cef776-42ce-4d3d-87f5-a09f77fbcd49",
   "metadata": {},
   "source": [
    "## Spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94310d-635a-4c29-b061-f031fb4ce0c7",
   "metadata": {},
   "source": [
    "* Por cuestiones de eficiencia y compatibilidad, Spark streaming guarda el stream en pequeños \"chunks\" ejecutando procesos batch (micro-batch)\n",
    "* input data stream => **Spark Streaming** => batches of input data => **Spark Engine** => batches of processed data\n",
    "* Un stream es representado como un stream discreto (DStream) el cual es una secuencia de RDDs\n",
    "* Cada RDD es un snapshot de todos los datos recolectados durante un período de tiempo, el cual luego se procesa como un batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd17af-e7cd-435a-afe9-6bbd9121a9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Ejemplo1\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Split the lines into words\n",
    "# Explode desagrega. ej: pp ; [a, b, c]  => pp;a  pp;b  pp;c\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Imprime en pantalla el procesamiento\n",
    "query = wordCounts.writeStream.outputMode(\"complete\").format(\"console\").option(\"numRows\", \"1000\").start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142c86e-5d5d-4e15-8387-8654c3cd4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Ejemplo1\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "# Split the lines into words\n",
    "temp = lines.select(explode(split(lines.value, \" \")).alias(\"temp\"))\n",
    "\n",
    "# Generate running word count\n",
    "temperaturas = temp.filter(\"temp > 25\").groupBy(\"temp\").count()\n",
    "\n",
    "query = temperaturas.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6b730-8133-431f-aa57-9ad7f7981f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import time\n",
    "import random\n",
    "\n",
    "s = socket.socket()\n",
    "\n",
    "port = 9999\n",
    "\n",
    "s.bind(('127.0.0.1', port))\n",
    "\n",
    "s.listen(5)\n",
    "i = 0\n",
    "while True:\n",
    "    c, addr = s.accept()\n",
    "    while True:    \n",
    "        c.send((str(random.randint(0, 40)) + \"\\r\").encode())\n",
    "        time.sleep(1)\n",
    "        print(\"pasa\")\n",
    "    c.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
