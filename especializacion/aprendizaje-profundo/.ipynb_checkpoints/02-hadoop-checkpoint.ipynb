{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938c0268-79a7-469b-9260-108ed43474ee",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c5d75-c6be-4a88-a628-fa7e57e05240",
   "metadata": {},
   "source": [
    "* Es un framework para procesar grandes volúmenes de datos en un ambiente distribuido\n",
    "* Actualmente pertenece a Apache\n",
    "* Incluye sistema de archivos distribuidos (HDFS)\n",
    "* Tolerante a fallas\n",
    "* Diseñado para el procesamiento off-line de los datos (procesamiento batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df4f59c-7a80-4e5d-9427-845ed5858c60",
   "metadata": {},
   "source": [
    "## Componentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7027b63-a951-43c4-acea-3d08e0f7913d",
   "metadata": {},
   "source": [
    "* Common (I/O, serialización, RPC)\n",
    "* HDFS (file system distribuido)\n",
    "* Zookeeper (servicio de coordinación de procesos)\n",
    "* MapReduce (modelo de procesamiento de datos)\n",
    "* Pig (lenguaje de scripting sobre MapReduce)\n",
    "* Cascading (framework que simplifica el uso de MapReduce)\n",
    "* Hive (lenguaje basado en SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a5427-91e0-42bc-ad6c-41a3cc3bde71",
   "metadata": {},
   "source": [
    "## DFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b152dc-5298-4135-b359-afa3427cc84f",
   "metadata": {},
   "source": [
    "* Sistema de archivos distribuidos\n",
    "* Ofrece transparencia al usuario permitiendo operar con todos los archivos del cluster a través del file system distribuido\n",
    "* Un mismo archivo podría estar almacenado en varias computadoras\n",
    "* Hay varios:\n",
    "    * HDFS\n",
    "    * HFTP\n",
    "    * HSFTP\n",
    "    * HAR\n",
    "    * FTP\n",
    "    * S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4ebb7-6d9d-497c-a8aa-bf9406d0ff4c",
   "metadata": {},
   "source": [
    "## HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0047bc-01c5-4154-ac2e-4967998308c9",
   "metadata": {},
   "source": [
    "* Sistema de archivos distribuidos Hadoop\n",
    "* Todos los archivos se dividen en bloques del mismo tamaño (64MB por defecto, aunque es configurable)\n",
    "* Los bloques pueden estar físicamente en cualquier computadora\n",
    "* Permite la réplica de bloques para optimización y recupero de fallas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c847a42a-2bca-4b60-9212-f9fd82f2cf4d",
   "metadata": {},
   "source": [
    "## Procesos del HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367b3f2-28ca-4324-9b53-381ffca1302d",
   "metadata": {},
   "source": [
    "* Namenode\n",
    "    * Maneja el árbol del filesystem y los metadatos de cada archivo y carpeta\n",
    "    * Conoce para cada bloque del FS que datanode lo maneja\n",
    "    * Vínculo con el filesystem del SO\n",
    "* Datanode del SO\n",
    "    * Son lo que llevan a cabo la lectura y escritura de los bloques en el filesystem\n",
    "    * Lleva a cabo la creación, borrado y replicado de los bloques\n",
    "* Secondary namenode: realiza tareas auxiliares al name node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cd191-e5f2-4cf0-bd45-025c44abf458",
   "metadata": {},
   "source": [
    "## Comandos HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96e57e-c8b9-4aa6-bf4a-573e2190a738",
   "metadata": {},
   "source": [
    "* HDFS permite crear, borrar, renombrar archivos y carpetas dentro del FS distribuido\n",
    "* Copiar un archivo del FS local al HDFS\n",
    "* Copiar un archivo del HDFS al FS local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e8ed2-2244-4890-85bc-e709a1e38602",
   "metadata": {},
   "source": [
    "```\n",
    "hadoop$ sbin/start-all.sh                     # inicia servicios\n",
    "hadoop$ cd bin\n",
    "hadoop$ ./hdfs dfs -ls                        # lista archivos\n",
    "hadoop$ ./hdfs dfs -df -h                     # muestra espacio libre\n",
    "hadoop$ ./hdfs dfs -put algo.txt datos/       # guarda archivo: origen -> destino\n",
    "hadoop$ ./hdfs dfs -get data3.txt c:/win/user # descarga archivo: origen -> destino\n",
    "hadoop$ ./hdfs dfs -cat algo.txt | more       # visualiza contenido del archivo q: para salir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574ede6-b17d-4c0e-88ed-93429101c1b3",
   "metadata": {},
   "source": [
    "## Componentes Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3a138-a87f-4834-9607-5749c9befa62",
   "metadata": {},
   "source": [
    "* En Hadoop la administración de los procesos que se ejecutan en el cluster la lleva a cabo un framework llamado Yarn MapReduce\n",
    "* CLUSTER: conjunto de nodos/máquinas conectadas en red\n",
    "* Básicamente Yarn realiza los trabajos usando dos procesos diferentes:\n",
    "    * **Job tracker**: maneja todos los trabajos a ser procesados. Tiene en cuenta el mapa del cluster al momento de crear los procesos Task\n",
    "    * **Task tracker**: son los encargados de realizar el procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0354c55-b2f5-428b-9bbc-528af2f3b20c",
   "metadata": {},
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f320ac-475f-4d52-970f-659f68635f5e",
   "metadata": {},
   "source": [
    "* Es un framework para distribuir tareas en múltiples nodos\n",
    "* TAREA --> CLUSTER (divide y venceras) distribuye a muchos nodos (físicos o virtuales)\n",
    "* Ventajas\n",
    "    * Paralelización y distribución de tareas automática\n",
    "    * Escalable\n",
    "    * Tolerante a fallos\n",
    "    * Monitoreo y capacidad de seguridad\n",
    "    * Flexibilidad de programación (Java, Python, C#, Ruby, C++)\n",
    "    * Abstracción al programador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94443a63-b52a-4d9b-904a-ccd7c8abab32",
   "metadata": {},
   "source": [
    "## Fase MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ea19a-2503-440a-9012-64ff35df0af2",
   "metadata": {},
   "source": [
    "* **Fase map**: en la que los datos de entrada son procesados, uno a uno, y **transformados** en un conjunto intermedio de datos.\n",
    "* **Fase reduce**: se reúnen los resultados intermedios y se reducen a un conjunto de datos resumidos, que es el resultado final de la tarea. Se aplican reglas (contar o compactar para que se vea poco)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bfb28-da08-48ce-8bf5-5efeccdc962c",
   "metadata": {},
   "source": [
    "## Ejemplo MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10efd0d0-0552-4ba2-a13a-c2e802392e00",
   "metadata": {},
   "source": [
    "* Map\n",
    "Pedirle a cada nodo que sume y cuente sus datos\n",
    "\n",
    "* Reduce\n",
    "\n",
    "```\n",
    "acum = 0; n = 0\n",
    "for nodo in cluster:\n",
    "    acum = acum + nodo.acum\n",
    "    n = n + nodo.n\n",
    "promedio = acum / n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7fc94-7ad2-4b19-9fc8-8e36bb47e214",
   "metadata": {},
   "source": [
    "* La unidad de trabajo de MapReduce es un Job\n",
    "* Un Job se divide en una tarea map y una tarea reduce\n",
    "* Los Jobs de MapReduce son controlados por un daemon conocido como JobTracker, el cual reside en el \"nodo master\"\n",
    "* Los clientes envían Jobs MapReduce al JobTracker y este distribuye la tarea usando otros nodos del cluster\n",
    "* Esos nodos se conocen como TaskTracker y son responsables de la ejecución de la tarea asignada y reportar el progreso al JobTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5d0cb-40fe-4770-82e0-908ee671a9be",
   "metadata": {},
   "source": [
    "## Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d60796-9ea9-48e5-94f2-51b90d37846d",
   "metadata": {},
   "source": [
    "* Herramienta que permite hacer procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f93be53-5c14-4619-be21-e1bf9703b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d5771f-b165-433f-ba5b-31ac1ee4bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/openjdk-17.jdk/Contents/Home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1393b74d-1eaf-4eb6-ad71-19d841c19552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0f9b7d5-c76e-4365-9e71-0084a609f793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/10 20:09:04 WARN Utils: Your hostname, MacBook-Pro-de-Pablo.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.84 instead (on interface en0)\n",
      "25/08/10 20:09:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/10 20:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate() # con * le digo que use todos los nucleos de mi máquina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da33676-1097-4899-bf2d-71b7a48ec227",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e6a02c-7504-4a6c-82d0-b5396aa0fd7b",
   "metadata": {},
   "source": [
    "* RDD (es una lista)\n",
    "    * R: Robusto\n",
    "    * D: Distribuido\n",
    "    * D: Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33d116b-1c2b-415f-b910-1e08f99bc5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eab6756-e88f-4914-990e-31f53f3412e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb98ad0-f97e-4ef2-9100-bd7de015ceec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3c3984-cdc7-4e55-afef-6f63449de90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 10, mean: 5.5, stdev: 2.8722813232690143, max: 10.0, min: 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.stats() # se está haciendo en forma paralela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5fe2f44-3e5c-40d2-b203-1660bdca9692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "658c2edc-5a3b-4383-8392-bcd019e90db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 5.5, 10], [5, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.histogram(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ebed11d-9a7c-4b59-b936-04f1d6730bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapeo sumar 1\n",
    "def sumar_uno(elemento):\n",
    "    return elemento + 1\n",
    "\n",
    "nuevo_rdd = rdd.map(sumar_uno) \n",
    "nuevo_rdd.collect()\n",
    "\n",
    "def reducir(x, y):\n",
    "    return x + y\n",
    "\n",
    "nuevo_rdd.reduce(reducir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6ee961-ed47-4ec5-83cf-16f10ee45167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtener la suma de todos los pares con map-reduce\n",
    "def pares(elemento):\n",
    "    if elemento % 2 == 0:\n",
    "        return elemento\n",
    "    return 0\n",
    "\n",
    "def sumar_pares(x, y):\n",
    "    return x + y\n",
    "\n",
    "nuevo_rdd = rdd.map(pares)\n",
    "nuevo_rdd.collect()\n",
    "\n",
    "nuevo_rdd.reduce(sumar_pares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a250503b-8a01-4a50-806a-af16b26fb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = spark.sparkContext.textFile('texto.txt') # frase es un rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09ba00ad-0e93-4493-ad02-6f728b3ad408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. Neque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. Ut enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? Quis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, vel illum qui dolorem eum fugiat quo voluptas nulla pariatur?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0e0fab9-5d5d-47f4-ad70-c4e6639b2750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ut', 3),\n",
       " ('perspiciatis', 1),\n",
       " ('unde', 1),\n",
       " ('omnis', 1),\n",
       " ('natus', 1),\n",
       " ('error', 1),\n",
       " ('doloremque', 1),\n",
       " ('laudantium,', 1),\n",
       " ('totam', 1),\n",
       " ('aperiam,', 1),\n",
       " ('quae', 1),\n",
       " ('illo', 1),\n",
       " ('inventore', 1),\n",
       " ('et', 2),\n",
       " ('architecto', 1),\n",
       " ('dicta', 1),\n",
       " ('Nemo', 1),\n",
       " ('enim', 2),\n",
       " ('aspernatur', 1),\n",
       " ('aut', 2),\n",
       " ('sed', 2),\n",
       " ('eos', 1),\n",
       " ('qui', 4),\n",
       " ('ratione', 1),\n",
       " ('porro', 1),\n",
       " ('dolorem', 2),\n",
       " ('dolor', 1),\n",
       " ('consectetur,', 1),\n",
       " ('non', 1),\n",
       " ('numquam', 1),\n",
       " ('eius', 1),\n",
       " ('modi', 1),\n",
       " ('tempora', 1),\n",
       " ('incidunt', 1),\n",
       " ('dolore', 1),\n",
       " ('magnam', 1),\n",
       " ('voluptatem.', 1),\n",
       " ('Ut', 1),\n",
       " ('veniam,', 1),\n",
       " ('quis', 1),\n",
       " ('nostrum', 1),\n",
       " ('exercitationem', 1),\n",
       " ('ullam', 1),\n",
       " ('corporis', 1),\n",
       " ('nisi', 1),\n",
       " ('ea', 2),\n",
       " ('commodi', 1),\n",
       " ('autem', 1),\n",
       " ('vel', 2),\n",
       " ('velit', 1),\n",
       " ('esse', 1),\n",
       " ('nihil', 1),\n",
       " ('molestiae', 1),\n",
       " ('consequatur,', 1),\n",
       " ('illum', 1),\n",
       " ('fugiat', 1),\n",
       " ('quo', 1),\n",
       " ('nulla', 1),\n",
       " ('pariatur?', 1),\n",
       " ('Sed', 1),\n",
       " ('iste', 1),\n",
       " ('sit', 3),\n",
       " ('voluptatem', 3),\n",
       " ('accusantium', 1),\n",
       " ('rem', 1),\n",
       " ('eaque', 1),\n",
       " ('ipsa', 1),\n",
       " ('ab', 1),\n",
       " ('veritatis', 1),\n",
       " ('quasi', 1),\n",
       " ('beatae', 1),\n",
       " ('vitae', 1),\n",
       " ('sunt', 1),\n",
       " ('explicabo.', 1),\n",
       " ('ipsam', 1),\n",
       " ('quia', 4),\n",
       " ('voluptas', 2),\n",
       " ('odit', 1),\n",
       " ('fugit,', 1),\n",
       " ('consequuntur', 1),\n",
       " ('magni', 1),\n",
       " ('dolores', 1),\n",
       " ('sequi', 1),\n",
       " ('nesciunt.', 1),\n",
       " ('Neque', 1),\n",
       " ('quisquam', 1),\n",
       " ('est,', 1),\n",
       " ('ipsum', 1),\n",
       " ('amet,', 1),\n",
       " ('adipisci', 1),\n",
       " ('velit,', 1),\n",
       " ('labore', 1),\n",
       " ('aliquam', 1),\n",
       " ('quaerat', 1),\n",
       " ('ad', 1),\n",
       " ('minima', 1),\n",
       " ('suscipit', 1),\n",
       " ('laboriosam,', 1),\n",
       " ('aliquid', 1),\n",
       " ('ex', 1),\n",
       " ('consequatur?', 1),\n",
       " ('Quis', 1),\n",
       " ('eum', 2),\n",
       " ('iure', 1),\n",
       " ('reprehenderit', 1),\n",
       " ('in', 1),\n",
       " ('voluptate', 1),\n",
       " ('quam', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def limpiar(elemento):\n",
    "    return elemento.split(' ')\n",
    "\n",
    "def agregar_claves(elemento):\n",
    "    return (elemento, 1)\n",
    "\n",
    "frase1 = frase.flatMap(limpiar)  # flatMap: siempre te da una lista\n",
    "con_claves = frase1.map(agregar_claves)\n",
    "final = con_claves.reduceByKey(reducir)\n",
    "final.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f6e959-289f-4c73-9150-dae0c96a8e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.saveAsTextFile('salida') # guarda en partes\n",
    "final.coalesce(1).saveAsTextFile('salida1') # en un solo archivo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
